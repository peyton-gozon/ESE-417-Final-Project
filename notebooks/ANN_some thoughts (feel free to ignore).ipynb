{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Applying the ANN Model to the Data.\n",
    "The goal of this notebook is to apply the MLP Classifier model to the data, and to optimize its performance. I will\n",
    "also put the entirety of the code in here, without applying PCA through the `data_loader`.\n",
    "\n",
    "\n",
    "### What was learned from `ANN.ipynb`?\n",
    "We saw that a largely un-tuned MLP Classifier can predict with $\\approx 64\\%$ accuracy. The goal is to greatly improve\n",
    "that metric.\n",
    "\n",
    "From `ANN.ipynb`, the top 10 models that we saw were as follows:\n",
    "```\n",
    "    accuracy activation  n_hidden_layers  n_nodes_per_layer solver\n",
    "12  0.643750       relu              2.0              279.0   adam\n",
    "20  0.643750       relu              4.0              139.0   adam\n",
    "9   0.639583   logistic              2.0              279.0  lbfgs\n",
    "16  0.639583       relu              3.0              186.0   adam\n",
    "7   0.631250       relu              1.0              100.0  lbfgs\n",
    "10  0.629167   logistic              2.0              279.0   adam\n",
    "19  0.627083       relu              4.0              139.0  lbfgs\n",
    "15  0.622917       relu              3.0              186.0  lbfgs\n",
    "8   0.620833       relu              1.0              100.0   adam\n",
    "5   0.616667   logistic              1.0              100.0  lbfgs\n",
    "```\n",
    "\n",
    "### Thoughts on how we may improve the model?\n",
    "* We should try evaluating models with 1-4 hidden layers.\n",
    "    * However, 3+ layer models have a problem of overtraining. We can account for this by performing a\n",
    "    train/test/validate split (instead of the normal train/test split). We will then train until we achieve a minimum on\n",
    "    the validation set.\n",
    "* It seems like stochastic solvers (`adam`) worked well with the `relu` activation function. Correspondingly, I'm going\n",
    "    to use SGD with Nesterov Momentum.\n",
    "    * I'm familiar with its inner workings from another class (Math 450), so it should be a pleasant optimization.\n",
    "* Statistical Normalization (Z-score) makes the most sense still, as PCA requires zero-mean data; it's also worth\n",
    "noting that NNs depend heavily on the scale of the data.\n",
    "* We _definitely_ need to optimize the learning rate. The slides recommend using $\\eta = 0.1$ as an initial value,\n",
    "however it will need to be annealed over time.\n",
    "    * It's typical to anneal the learning rate by $\\frac{1}{N_\\mathrm{iter}}$ when using SGD. I'm sure ADAM is similar.\n",
    "    * The initial learning rate used by `sklearn` is actually `0.001`, making this a huge difference.\n",
    "        * We can anneal this according to Wolfe's conditions via `sklearn`'s interface.\n",
    "* We need to perform some degree of optimization on the regularization parameter as well.\n",
    "    * The default regularization constant is $\\alpha = 0.0001$, which is likely too little to truly impact our\n",
    "        overfitting problem.\n",
    "\n",
    "### Define a data pipeline\n",
    "This will perform\n",
    "1. Apply Z-score standardization to the data\n",
    "2. Apply PCA with 8 components to the data\n",
    "3. Feed the data into the MLP Classifier.\n",
    "    * Note this uses SGD with early-stopping (which implicitly uses a validation set to combat overtraining)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from models.data_loader import DataLoader\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# We learned from `exploring_data.ipynb` that PCA with 8 principal components is optimal.\n",
    "n_components = 8\n",
    "\n",
    "# Random state to allow for this to be deterministic.\n",
    "random_state =  np.random.RandomState(42069)\n",
    "\n",
    "# Build the model pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('standardization', StandardScaler()),\n",
    "    ('pca', PCA(n_components=n_components, random_state=random_state)),\n",
    "    ('classifier', MLPClassifier(\n",
    "        random_state=random_state, activation='relu', solver='sgd', learning_rate='adaptive', early_stopping=True\n",
    "    ))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the data and create a test/train split."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dl = DataLoader('../data/winequality-red.csv', random_state=random_state)\n",
    "X_train, X_test, y_train, y_test = dl.train_test_split()\n",
    "\n",
    "N_train, d = X_train.shape\n",
    "\n",
    "N_h = N_train // 10  # Following the recommendation from the slides."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the search parameter grid space\n",
    "\n",
    "Parameters to change:\n",
    "* `hidden_layer_sizes`\n",
    "* `alpha` - regularization penalty\n",
    "* `learning_rate_init` - the initial learning rate\n",
    "* `momentum`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameter_grid = {\n",
    "    'classifier__hidden_layer_sizes': [\n",
    "        (N_h,), (N_h//2,N_h//2,), (N_h//3,N_h//3,N_h//3,), (N_h//4,N_h//4,N_h//4,N_h//4,)\n",
    "    ],\n",
    "    'classifier__alpha': np.logspace(-4, -2, 5),\n",
    "    'classifier__learning_rate_init': [0.1, 0.01, 0.001],\n",
    "    'classifier__momentum': [0, 0.9]\n",
    "}\n",
    "\n",
    "# 3-fold validation.\n",
    "grid_search = GridSearchCV(model_pipeline, param_grid=parameter_grid, scoring='roc_auc', n_jobs=-1, cv=3)\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}