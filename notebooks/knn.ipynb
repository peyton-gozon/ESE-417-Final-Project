{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# K-Nearest Neighbors\n",
    "\n",
    "K-NN is an effective model for performing classification, which may help us make sense of the clusters that we saw from\n",
    "`exploring_data.ipynb`. In particular\n",
    "![PCA with 2 Principal Components](/images/pca_2_components.png)\n",
    "and\n",
    "![PCA with 3 Principal Components](/images/pca_3_components.png).\n",
    "\n",
    "There is some concern with this. Chiefly, we did not see separateable clusters from the 2d PCA representation\n",
    "of the data, or from our vantage of the 3d representation. It is my hope, however, that there is more cluster separation\n",
    "offered in a higher dimension. We will see.\n",
    "\n",
    "### Goals\n",
    "1. Determine if cluster separation is viable in a higher dimension.\n",
    "    * This goes back to the point from above. The clusters appeared to be on top of each other in 2D and (possibly) 3D.\n",
    "    Perhaps this is not the case in a higher dimensional setting.\n",
    "2. Determine how effective K-NN will be on the data.\n",
    "    * This means that we are not tuning, but exploring a few sets of options to get a feel.\n",
    "3. Determine what distance metric appears to make the most sense.\n",
    "    * Euclidean distance does not make much intuitive sense when comparing wines. Perhaps there's a better metric.\n",
    "\n",
    "#### Implementing KNN\n",
    "As done in `SVM.ipynb`, I will perform KNN on the data after standardization and PCA has been applied, as it will\n",
    "generally improves performance to run KNN on a lower dimensional dataset. This may not be required, however, as we're\n",
    "looking at low-dimensional data as is ($d \\leq 11$), and a kernel would make this an $O(d)$ operation. We will use\n",
    "8 principal components for the time being, as that is what the exploratory analysis indicated to be effective.\n",
    "\n",
    "#### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from models.data_loader import DataLoader\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rs = np.random.RandomState(42069)\n",
    "\n",
    "# Load the data into memory and apply PCA with 8 principal components\n",
    "dl = DataLoader('../data/winequality-red.csv', random_state=rs)\n",
    "dl.apply_pca_to_dataset(8)\n",
    "\n",
    "# Apply a train/test split.\n",
    "X_train, X_test, y_train, y_test = dl.train_test_split()\n",
    "\n",
    "# Define the parameters of the model to explore.\n",
    "n_neighbors = [5, 25, 50, 100, 200, 500]  # The goal is to explore a wide set of neighbors to get some intuition.\n",
    "distance_metrics = [\n",
    "    'euclidean',   # L2 norm\n",
    "    'manhattan',   # L1 norm\n",
    "    'minkowski',   # Lp norm.\n",
    "    'chebyshev',   # max(|x - y|)\n",
    "]\n",
    "\n",
    "N_train,d = X_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Applying the exploratory Models to the Data.\n",
    "By taking a set product of $\\mathrm{n_neighbors} \\times \\mathrm{distance_metrics}$, we can feel out how well the model\n",
    "is performing, while also getting an idea of which distance metric(s) might make sense to explore further."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "performances = pd.DataFrame(columns=['n_neighbors', 'distance_metric', 'accuracy'])\n",
    "\n",
    "for k, distance_metric in itertools.product(n_neighbors, distance_metrics):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric=distance_metric, p=d)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Compute the model's accuracy on the testing data.\n",
    "    accuracy = knn.score(X_test, y_test)\n",
    "\n",
    "    performances = performances.append({\n",
    "        'n_neighbors': k,\n",
    "        'distance_metric': distance_metric,\n",
    "        'accuracy': accuracy\n",
    "    }, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Display the Top-10 Best Models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_neighbors distance_metric  accuracy\n",
      "5           25       manhattan  0.591667\n",
      "4           25       euclidean  0.589583\n",
      "10          50       minkowski  0.587500\n",
      "11          50       chebyshev  0.585417\n",
      "0            5       euclidean  0.577083\n",
      "7           25       chebyshev  0.577083\n",
      "12         100       euclidean  0.575000\n",
      "6           25       minkowski  0.575000\n",
      "14         100       minkowski  0.572917\n",
      "15         100       chebyshev  0.572917\n"
     ]
    }
   ],
   "source": [
    "performances = performances.sort_values('accuracy', ascending=False)\n",
    "print(performances.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### What was learned?\n",
    "* Using 25-50 nearest neighbors appeared to be most effective.\n",
    "    * this provides a good indication of where to begin with hyper parameter tuning, but it does not give us much\n",
    "    conclusive information.\n",
    "* All four distance metrics showed up in the top 5 model predictions, and all performed similarly.\n",
    "    * This is interesting, as all of these top performing models appeared within ~2% of each other. This implies a few things.\n",
    "        1. A different performance metric will be required to evaluate the models. Perhaps AUC would be a good idea.\n",
    "        2. The notion of distance used does not appear to have much impact on the model. In other words, the ways in\n",
    "            which we attempt to numerically classify which wines are the \"most similar\" does not appear to have a great\n",
    "            effect.\n",
    "            * Perhaps this has something to do with the data being standardized ahead of time; however, with distances\n",
    "            being relative, I do not believe this is an issue.\n",
    "        3. It may make sense to pursue other types of distance metrics. For example, Mahalanobis Distance, which has\n",
    "            some effective interpretations.\n",
    "\n",
    "#### Exploring Mahalanobis Distance\n",
    "Recommended reading: [Mahalanobis Distance](https://en.wikipedia.org/wiki/Mahalanobis_distance)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring using Mahalanobis Distance: \n",
      "n_neighbors\taccuracy\n",
      "5\t\t0.5770833333333333\n",
      "25\t\t0.5458333333333333\n",
      "50\t\t0.5708333333333333\n",
      "100\t\t0.5625\n",
      "200\t\t0.55625\n",
      "500\t\t0.5145833333333333\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Mahalanobis Distance requires the covariance matrix\n",
    "Sigma = np.cov(X_train)\n",
    "\n",
    "print(\"Exploring using Mahalanobis Distance: \")\n",
    "print(\"n_neighbors\\taccuracy\")\n",
    "for k in n_neighbors:\n",
    "    knn = KNeighborsClassifier(algorithm='brute', n_neighbors=k, metric='mahalanobis', metric_params={'V': Sigma})\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    accuracy = knn.score(X_test, y_test)\n",
    "    print(f\"{k}\\t\\t{accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Not much of a performance increase, unfortunately. In fact, it generally performed worse than the previous models, with\n",
    "the best accuracy only being $~57.71\\%$.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}