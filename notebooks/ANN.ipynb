{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ANN\n",
    "\n",
    "Neural Networks are known as \"universal approximators,\" meaning that they can learn _any_ function, no matter how\n",
    "complex, given the correct number of hidden layers + nodes/layer. This is primarily just to see how effective it could\n",
    "be, without much tuning.\n",
    "\n",
    "#### Goals:\n",
    "1. Determine how effective an ANN would be at this type of classification task.\n",
    "2. Get a general idea of what ballpark parameters should be in.\n",
    "\n",
    "I imagine that this will result in considerable overfitting and may not generalize too well. I am expecting, however,\n",
    "$> 60\\%$ accuracy from this model. I do not anticipate that this will perform much better than a RFC model.\n",
    "\n",
    "#### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.data_loader import DataLoader\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "rs = np.random.RandomState(42069)\n",
    "\n",
    "# Load the data.\n",
    "dl = DataLoader('../data/winequality-red.csv', random_state=rs)\n",
    "dl.apply_pca_to_dataset()\n",
    "\n",
    "# Apply a Train/Test split.\n",
    "X_train, X_test, y_train, y_test = dl.train_test_split()\n",
    "\n",
    "# Obtain the dimension of the data.\n",
    "N_train, d = X_train.shape\n",
    "\n",
    "N_h = N_train // 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Building the MLP Classifier\n",
    "There are a couple of parameters to consider.\n",
    "* `hidden_layer_sizes`: tuple containing how many nodes to have for each hidden layer\n",
    "* `activation`: which activation function to use. I'm a fan of `logistic` (sigmoid), but `ReLU` is good too.\n",
    "* `solver`: the `lbfgs` solver is a quasi-Newton method that works well on small datasets. However, we can explore `adam`\n",
    "as well, which is a type of SGD.\n",
    "\n",
    "Notes: There are many other parameters to tune, including learning rate, regularization penalty, etc. However, these will\n",
    "be saved for if/when we select the ANN as a final model.\n",
    "\n",
    "Like before, the set of parameter setups we're exploring is $\\mathrm{hidden\\_layer\\_sizes} \\times \\mathrm{activation} \\times \\mathrm{solver}$.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [\n",
    "    (100,), # This is the default\n",
    "    # There's a recommendation for the number of hidden nodes to be 1/10th of the total number of examples.\n",
    "    # See `slides_module6.pdf` page 32. I am exploring that option here, with a 2-layer, 3-layer, and 4-layer NN.\n",
    "    (N_h//2, N_h//2,),\n",
    "    (N_h//3, N_h//3, N_h//3,),\n",
    "    (N_h//4, N_h//4, N_h//4, N_h//4,),\n",
    "]\n",
    "activation = ['logistic', 'relu']\n",
    "solvers = ['lbfgs', 'adam']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peyto\\anaconda3\\envs\\scientificProject\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\peyto\\anaconda3\\envs\\scientificProject\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "performance = pd.DataFrame(index=['n_hidden_layers', 'n_nodes_per_layer', 'activation', 'solver', 'accuracy'])\n",
    "\n",
    "for hidden_layer_setup, activation_fn, solver in itertools.product(hidden_layer_sizes, activation, solvers):\n",
    "    # Build and fit the model.\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_setup, activation=activation_fn, solver=solver, random_state=rs,\n",
    "                        max_iter=2000)\n",
    "    mlp.fit(X_train, y_train)\n",
    "\n",
    "    # Score the model on the test set.\n",
    "    accuracy = mlp.score(X_test, y_test)\n",
    "\n",
    "    # Log its performance\n",
    "    performance = performance.append({\n",
    "        'n_hidden_layers': len(hidden_layer_setup),\n",
    "        'n_nodes_per_layer': hidden_layer_setup[0],\n",
    "        'activation': activation_fn,\n",
    "        'solver': solver,\n",
    "        'accuracy': accuracy\n",
    "    }, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Determine the Top-10 Best Model Configurations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    accuracy activation  n_hidden_layers  n_nodes_per_layer solver\n",
      "12  0.643750       relu              2.0              279.0   adam\n",
      "20  0.643750       relu              4.0              139.0   adam\n",
      "9   0.639583   logistic              2.0              279.0  lbfgs\n",
      "16  0.639583       relu              3.0              186.0   adam\n",
      "7   0.631250       relu              1.0              100.0  lbfgs\n",
      "10  0.629167   logistic              2.0              279.0   adam\n",
      "19  0.627083       relu              4.0              139.0  lbfgs\n",
      "15  0.622917       relu              3.0              186.0  lbfgs\n",
      "8   0.620833       relu              1.0              100.0   adam\n",
      "5   0.616667   logistic              1.0              100.0  lbfgs\n"
     ]
    }
   ],
   "source": [
    "performance = performance.sort_values('accuracy', ascending=False)\n",
    "print(performance.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### What was learned?\n",
    "* It performs better than the SVM + KNN models (without much tuning), but it's outclassed by the Random Forest model.\n",
    "* The model appears to not always converge when using a stochastic optimizer (even within 2000 iterations), meaning that\n",
    "    either the data is mal-formatted internally or this will become a very computationally expensive operation.\n",
    "* While I do not have definitive proof, I'm assuming that there is considerable overfitting occurring (as is the nature\n",
    "of MLPs). If we do proceed with this model, we _will_ need to focus on annealing the learning rate properly + tuning the\n",
    "    regularization parameter.\n",
    "* The prototypical `ReLU` activation function appears to be the best."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}