{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "While I'm no wine connoisseur, I do have to question how formulaic it is to find the \"best\" wine. For example, perhaps\n",
    "a wine with quality 8 has a particular range of pH and sulfur content. I do not know, but that's what this model should\n",
    "tell us more about.\n",
    "\n",
    "#### Goals\n",
    "1. Determine the effectiveness of a random forest model on the wine dataset\n",
    "    * Apply a few sets of hand-selected values for each hyper parameter to get a feel for the dataset.\n",
    "2. Attempt to draw some conclusions regarding whether random forest is the way to go.\n",
    "\n",
    "There's not really much to say here. I'm expecting $~60\\%$ performance like we've been seeing with most of these models.\n",
    "\n",
    "#### Loading the Data\n",
    "As before, we're going to load the data and apply PCA with the results learned from `exploring_data.ipynb`. Reducing the\n",
    "dimension from $11 \\to 8$ should help us remove some of the inherent noise in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from models.data_loader import DataLoader\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "rs = np.random.RandomState(42069)\n",
    "\n",
    "# Load the data.\n",
    "dl = DataLoader('../data/winequality-red.csv', random_state=rs)\n",
    "dl.apply_pca_to_dataset()\n",
    "\n",
    "# Apply a Train/Test split.\n",
    "X_train, X_test, y_train, y_test = dl.train_test_split()\n",
    "\n",
    "# Obtain the dimension of the data.\n",
    "_, d = X_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Applying the Random Forest model to the Data\n",
    "The main hyperparameters to explore are:\n",
    "1. `n_estimators`: the number of trees in the forest\n",
    "2. `max_depth`: how many layers deep each tree may go\n",
    "3. `class_balance`: either `balanced` or `balanced_subsample`. You can read more here: [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "\n",
    "I'm going to explore these values initially:\n",
    "* `n_estimators` $\\in \\{ 20, 50, 100, 200, 300 \\}$.\n",
    "* `max_depth`: $\\in \\{ \\mathrm{None}, d, d^2\\}$, with $d$-dimensional data. Note that `None` allows for as much\n",
    "depth as is needed, and is the default value.\n",
    "* Both `class_balance` options.\n",
    "\n",
    "This is intentionally sparse, as the goal is not to perform hypertuning, but to evaluate what to expect with the model.\n",
    "\n",
    "Again, the set of all options being explored here is $\\mathrm{n\\_estimators} \\times \\mathrm{max\\_depth} \\times \\mathrm{class\\_balance}$.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "n_estimators = [20, 50, 100, 200, 300]\n",
    "max_depths = [None, d, d**2]\n",
    "class_balances = ['balanced', 'balanced_subsample']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "performances = pd.DataFrame(index=['n_estimators', 'max_depth', 'class_balance', 'accuracy'])\n",
    "\n",
    "for n, max_depth, class_balance in itertools.product(n_estimators, max_depths, class_balances):\n",
    "    rfc = RandomForestClassifier(n_estimators=n, max_depth=max_depth, class_weight=class_balance, random_state=rs)\n",
    "    rfc.fit(X_train, y_train)\n",
    "\n",
    "    accuracy = rfc.score(X_test, y_test)\n",
    "\n",
    "    performances = performances.append({\n",
    "        'n_estimators': n,\n",
    "        'max_depth': max_depth,\n",
    "        'class_balance': class_balance,\n",
    "        'accuracy': accuracy\n",
    "    }, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Determine the Top-10 Best Models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    accuracy       class_balance max_depth  n_estimators\n",
      "32  0.706250            balanced        64         300.0\n",
      "27  0.702083  balanced_subsample        64         200.0\n",
      "33  0.697917  balanced_subsample        64         300.0\n",
      "29  0.697917  balanced_subsample      None         300.0\n",
      "22  0.697917            balanced      None         200.0\n",
      "17  0.697917  balanced_subsample      None         100.0\n",
      "20  0.695833            balanced        64         100.0\n",
      "23  0.695833  balanced_subsample      None         200.0\n",
      "26  0.691667            balanced        64         200.0\n",
      "10  0.689583            balanced      None          50.0\n"
     ]
    }
   ],
   "source": [
    "performances = performances.sort_values('accuracy', ascending=False)\n",
    "print(performances.head(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### What was Learned?\n",
    "* The Random Forest Classifier model performed much better than I anticipated, i.e., about 10% better.\n",
    "* It appears to favor a larger depth, with many of the best performing models using a `max_depth` of $d^2$ or `None`.\n",
    "* The class balance method does not appear to have much impact, with half of the models using each method in the top 10.\n",
    "    * 3 of the 5 top models use `balanced_subsample`, though that's hardly worth drawing conclusions from.\n",
    "* It would be wise to see how well it performs with other performance measurements, such as AUC, when applying the final\n",
    "model.\n",
    "* Using many estimators appears be favorable, albeit more computationally expensive. It would make sense to view the\n",
    "confusion matrices for these top estimators."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}